# Модель маршрутизации трафика с учетом равновесия Уордаропа и машинного обучения

Этот проект представляет собой симуляцию распределения трафика на дорожной сети с использованием теории игр и алгоритма Q-learning. 

Цель кода — продемонстрировать, как водители могут выбирать маршруты на основе разных факторов: время в пути, стоимость, комфорт и как распределение трафика может адаптироваться к различным внешним условиям - аварии, перекрытия дорог, пока система не достигнет состояния равновесия Уордаропа.

## Основные элементы проекта:

### 1) Модель дорожной сети:

Каждая дорога имеет параметры:

- base_time: базовое время в пути без учета трафика
- traffic_factor: коэффициент увеличения времени в зависимости от трафика
- quality: качество дороги, влияющее на пропускную способность
- cost: стоимость проезда по дороге
- comfort: уровень комфорта дороги для водителей

Пример дорог: A, B, C, D, E.

### 2) Водители и их предпочтения:

Водители оценивают маршруты на основе:

- Времени в пути.
- Стоимости маршрута.
- Уровня комфорта.

Каждый фактор имеет свой вес в оценке маршрута, что позволяет моделировать личные предпочтения водителей (например, один водитель может больше ценить комфорт, а другой — низкую стоимость).

### 3) Аварийные ситуации:

Для каждой дороги случайным образом могут возникать аварийные ситуации:

- Полное закрытие дороги.
- Частичное замедление движения.
- Отсутствие аварий (нормальные условия).

В зависимости от аварии, время в пути на этой дороге может увеличиваться или дорога может стать полностью недоступной.

### 4) Перераспределение трафика:

Водители перераспределяют свои маршруты, выбирая те дороги, которые минимизируют их общее недовольство - время, стоимость и комфорт.
Для перераспределения трафика используется функция redistribute_traffic(), которая учитывает текущую нагрузку на дороги и новые оценки маршрутов после аварий.

### 5) Q-learning для адаптации водителей:

Реализован алгоритм Q-learning, который позволяет водителям постепенно улучшать свои стратегии выбора маршрутов.

В каждом раунде водители обучаются на основе оценок маршрутов и наград, чтобы выбрать наилучший маршрут в будущем.

### 6) Q-learning настраивается через параметры:

- alpha: скорость обучения
- gamma: коэффициент дисконтирования будущих вознаграждений
- epsilon: вероятность выбора случайного действия для исследования новых маршрутов

### 7) Равновесие Уордаропа:

Система проверяет, достигнуто ли равновесие Уордаропа, то есть состояние, при котором ни один водитель не может улучшить свое положение, сменив маршрут.
Если изменения в распределении трафика становятся достаточно малыми, процесс останавливается.

#### Запуск симуляции:

Код выполняется в несколько раундов. В каждом раунде происходит следующее:
Моделирование аварий и пересчет времени в пути для всех дорог.

Оценка маршрутов водителями с учетом времени, стоимости и комфорта.
Перераспределение трафика на основе новых оценок маршрутов.

Обучение с использованием Q-learning для выбора оптимальных маршрутов в будущем.

Симуляция продолжается до тех пор, пока не будет достигнуто равновесие или не закончится количество раундов.

#### Возможные улучшения:

Учет личных предпочтений водителей: Водители могут принимать решения на основе более сложных факторов, таких как погодные условия или динамические тарифы.

Более сложные сценарии аварий: Моделирование разных типов аварий, которые могут иметь разные последствия для трафика.

Машинное обучение: Алгоритмы, такие как Q-learning, могут быть улучшены с добавлением дополнительных состояний и действий для более точного моделирования поведения водителей.

#### Пояснение визуализации:

Изменение времени на дорогах: 
Этот график показывает, как изменяется время в пути на каждой из дорог в зависимости от трафика на них в течение всех раундов симуляции.

Изменение трафика на дорогах: 
На этом графике можно увидеть, как распределяется трафик по дорогам с течением времени, что наглядно демонстрирует перераспределение водителей в зависимости от их предпочтений и условий дорог.

Изменение средних наград водителей: 
Этот график демонстрирует, как изменяется средняя награда (удовлетворение) водителей по мере того, как они обучаются выбирать более оптимальные маршруты с помощью Q-learning.

#### Инструкция по запуску:

Скачайте код.
Убедитесь, что у вас установлены необходимые библиотеки:

bash
pip install numpy
python traffic_simulation.py


Наблюдайте за результатами симуляции и процессом достижения равновесия.
